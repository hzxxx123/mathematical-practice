---
title: "homework3"
author: "黄舟翔 3220103606"
date: "2025-06-28"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
    extra_dependencies: ["ctex"]
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{Times New Roman}
---
```{r setup, message = F, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE)  # 全局隐藏所有警告
```
**Background**: In the previous lectures and lab, we fitted the following model
\[
 Y = y_0 N^a + \mathrm{noise}
\]
by minimizing the mean squared error
\[
 \frac{1}{n}\sum_{i=1}^{n}{(Y_i - y_0 N_i^a)^2}.
\]

We did this by approximating the derivative of the MSE, and adjusting $a$ by an amount proportional to that, stopping when the derivative became small.  Our procedure assumed we knew $y_0$.  In this assignment, we will use a built-in R function to estimate both parameters at once; it uses a fancier version of the same idea.

Because the model is nonlinear, there is no simple formula for the parameter estimates in terms of the data.  Also unlike linear models, there is no simple formula for the _standard errors_ of the parameter estimates.  We will therefore use a technique called **the jackknife** to get approximate standard errors.

Here is how the jackknife works:

* Get a set of $n$ data points and get an estimate $\hat{\theta}$ for the  parameter of interest $\theta$.
* For each data point $i$, remove $i$ from the data set, and get an  estimate $\hat{\theta}_{(-i)}$ from the remaining $n-1$ data points.  The $\hat{\theta}_{(-i)}$ are sometimes called the "jackknife estimates".
* Find the mean $\overline{\theta}$ of the $n$ values of $\hat{\theta}_{(-i)}$
* The jackknife variance of $\hat{\theta}$ is
  \[
  \frac{n-1}{n}\sum_{i=1}^{n}{(\hat{\theta}_{(-i)} - \overline{\theta})^2} = \frac{(n-1)^2}{n}\mathrm{var}{[\hat{\theta}_{(-i)}]}
  \]
where $\mathrm{var}$ stands for the sample variance.  (_Challenge_: can you explain the factor of $(n-1)^2/n$?  _Hint_: think about what happens when $n$ is large so $(n-1)/n \approx 1$.)
* The jackknife standard error of $\hat{\theta}$ is the square root of the jackknife variance.
  
You will estimate the power-law scaling model, and its uncertainty, using the data alluded to in lecture, available in the file `gmp.dat` from lecture, which contains data for 2006.
```
gmp <- read.table("gmp.dat")
gmp$pop <- round(gmp$gmp/gmp$pcgmp)
```

### 1. 

First, plot the data as in lecture, with per capita GMP on the y-axis and population on the x-axis. Add the curve function with the default values provided in lecture. Add two more curves corresponding to $a=0.1$ and $a=0.15$; use the `col` option to give each curve a different color (of your choice).

用以下代码完成

```{r}
gmp <- read.table("data/gmp.dat", col.names = c("MSA", "gmp", "pcgmp"))
gmp$pop <- round(gmp$gmp / gmp$pcgmp)

# 绘制散点图
plot(pcgmp ~ pop, data = gmp, log = "xy", 
     xlab = "Population", ylab = "Per Capita GMP",
     main = "Power Law Scaling (2006)")

# 添加三条理论曲线
curve(6611 * x^(1/8), add = TRUE, col = "blue", lwd = 2)      # 默认值
curve(6611 * x^0.10, add = TRUE, col = "red", lwd = 2)        # a=0.1
curve(6611 * x^0.15, add = TRUE, col = "darkgreen", lwd = 2)  # a=0.15
legend("topright", legend = c("y0=6611, a=0.125", "y0=6611, a=0.10", "y0=6611, a=0.15"),
       col = c("blue", "red", "darkgreen"), lwd = 2)
```



### 2.

Write a function, called `mse()`, which calculates the mean squared error of the model on a given data set. `mse()` should take three arguments: a numeric vector of length two, the first component standing for $y_0$ and the second for $a$; a numerical vector containing the values of $N$; and a numerical vector containing the values of $Y$.  The function should return a single numerical value. The latter two arguments should have as the default values the columns `pop` and `pcgmp` (respectively) from the `gmp` data frame from lecture.  Your function may not use `for()` or any other loop. Check that, with the default data, you get the following values.

用以下代码解决，经验证输出符合预期。
```{r}
mse <- function(params, N = gmp$pop, Y = gmp$pcgmp) {
  y0 <- params[1]
  a <- params[2]
  predictions <- y0 * (N^a)
  mean((Y - predictions)^2)
}

# 验证函数输出
mse(c(6611, 0.15))  # 应返回 ~207057513
mse(c(5000, 0.10))  # 应返回 ~298459915
```
```
> mse(c(6611,0.15))
[1] 207057513
> mse(c(5000,0.10))
[1] 298459915
```

### 3. 

R has several built-in functions for optimization, which we will meet as we go through the course.  One of the simplest is `nlm()`, or non-linear minimization. `nlm()` takes two required arguments: a function, and a starting value for that function. Run `nlm()` three times with your function `mse()` and three starting value pairs for $y0$ and $a$ as in
```
nlm(mse, c(y0=6611,a=1/8))
```
What do the quantities `minimum` and `estimate` represent? What values does it return for these?

用以下代码解决, minimum表示最小化的MSE值，estimate:表示参数估计值(y0, a)


```{r}
fit1 <- nlm(mse, c(y0 = 6611, a = 1/8))
fit2 <- nlm(mse, c(y0 = 6611, a = 0.15))
fit3 <- nlm(mse, c(y0 = 5000, a = 0.10))

# 解释结果:

list(
  fit1 = list(minimum = fit1$minimum, estimate = fit1$estimate),
  fit2 = list(minimum = fit2$minimum, estimate = fit2$estimate),
  fit3 = list(minimum = fit3$minimum, estimate = fit3$estimate)
)
```

### 4.

Using `nlm()`, and the `mse()` function you wrote, write a function, `plm()`, which estimates the parameters $y_0$ and $a$ of the model by minimizing the mean squared error.  It should take the following arguments: an initial guess for $y_0$; an initial guess for $a$; a vector containing the $N$ values; a vector containing the $Y$ values.  All arguments except the initial guesses should have suitable default values.  It should return a list with the following components: the final guess for $y_0$; the final guess for $a$; the final value of the MSE.  Your function must call those you wrote in earlier questions (it should not repeat their code), and the appropriate arguments to `plm()` should be passed on to them.  
What parameter estimate do you get when starting from $y_0 = 6611$ and $a = 0.15$?  From $y_0 = 5000$ and $a = 0.10$?  If these are not the same, why do they differ?  Which estimate has the lower MSE?  

用以下代码解决，经比较，$y_0 = 6611$ 和 $a = 0.15$ 的mse值更小。
幂律模型 $Y = y_0 N^a$ 是一个非线性模型。其均方误差(MSE)函数关于参数 $y_0$ 和 $a$ 是非凸的（不是光滑的碗状）。这意味着存在多个局部最小值点，而非一个全局最小值。优化算法可能收敛到不同的局部最小值点，取决于起始位置。
nlm() 函数使用基于梯度的方法进行优化。这种方法容易陷入局部最小值，特别是当起始点远离全局最优解时，算法会沿着最陡下降方向移动，可能无法"跳出"局部最优区域。

```{r}
plm <- function(y0_init, a_init, N = gmp$pop, Y = gmp$pcgmp) {
  result <- nlm(mse, c(y0_init, a_init), N = N, Y = Y)
  list(
    y0 = result$estimate[1],
    a = result$estimate[2],
    mse = result$minimum
  )
}

# 测试不同初始值
fit_opt1 <- plm(6611, 0.15)
fit_opt2 <- plm(5000, 0.10)

# 比较结果
fit_opt1  # 显示第一个估计
fit_opt2  # 显示第二个估计
# 输出MSE值比较
fit_opt1$mse < fit_opt2$mse  # 判断哪个MSE更小
```


### 5. _Convince yourself the jackknife can work_.
#### a. 

Calculate the mean per-capita GMP across cities, and the standard error of this mean, using the built-in functions `mean()` and `sd()`, and the formula for the standard error of the mean you learned in your intro. stats. class (or looked up on Wikipedia...).

直接计算均值标准误差

```{r}
mean_pcgmp <- mean(gmp$pcgmp)
n <- nrow(gmp)
se_direct <- sd(gmp$pcgmp) / sqrt(n)
print(se_direct)
```
#### b. 

Write a function which takes in an integer `i`, and calculate the mean per-capita GMP for every city _except_ city number `i`.

函数代码如下：

```{r}
mean_excluding_i <- function(i) {
  subset_data <- gmp$pcgmp[-i]
  mean(subset_data)
}
```
#### c. 

Using this function, create a vector, `jackknifed.means`, which has the mean per-capita GMP where every city is held out in turn.  (You may use a `for` loop or `sapply()`.)

用以下代码完成：

```{r}
n <- nrow(gmp)
jackknife.means <- sapply(1:n, mean_excluding_i)
summary(jackknife.means)
cat("\n刀切法均值向量:\n")
print(jackknife.means)
```


#### d. 

Using the vector `jackknifed.means`, calculate the jack-knife approximation to the standard error of the mean.  How well does it match your answer from part (a)?

用以下代码完成，计算结果发现，相对误差绩效，说明这种方法是可行的。

```{r}
mean_jk <- mean(jackknife.means)
se_jackknife <- sqrt(((n-1)/n) * sum((jackknife.means - mean_jk)^2))

# 比较结果
cat("直接计算标准误:", se_direct, "\n刀切法标准误:", se_jackknife, "\n相对差异:", 
    abs(se_direct - se_jackknife)/se_direct * 100, "%\n")
```


### 6. 

Write a function, `plm.jackknife()`, to calculate jackknife standard errors for the parameters $y_0$ and $a$.  It should take the same arguments as `plm()`, and return standard errors for both parameters.  This function should call your `plm()` function repeatedly.  What standard errors do you get for the two parameters?

用以下代码完成

```{r}
plm.jackknife <- function(y0_init, a_init, N = gmp$pop, Y = gmp$pcgmp) {
  n <- length(N)
  jk_y0 <- numeric(n)
  jk_a <- numeric(n)
  
  for (i in 1:n) {
    fit <- plm(y0_init, a_init, N[-i], Y[-i])
    jk_y0[i] <- fit$y0
    jk_a[i] <- fit$a
  }
  
  # 计算刀切法方差
  var_y0 <- ((n-1)^2 / n) * var(jk_y0)
  var_a <- ((n-1)^2 / n) * var(jk_a)
  
  list(se_y0 = sqrt(var_y0), se_a = sqrt(var_a))
}

# 计算2006年参数标准误
se_2006 <- plm.jackknife(6611, 0.15)
se_2006
```

### 7. 

The file `gmp-2013.dat` contains measurements for 2013.  Load it, and use `plm()` and `plm.jackknife` to estimate the parameters of the model for 2013, and their standard errors.  Have the parameters of the model changed significantly?

用以下代码完成，结果显示，$y_0$的变化不显著，而a的变化相对较为显著。

```{r}
gmp2013 <- read.table('data/gmp-2013.dat', header = TRUE, 
                      col.names = c("MSA", "gmp", "pcgmp"))
gmp2013$pop <- round(gmp2013$gmp / gmp2013$pcgmp)

# 拟合2013年模型
fit_2013 <- plm(6611, 0.15, gmp2013$pop, gmp2013$pcgmp)
se_2013 <- plm.jackknife(6611, 0.15, gmp2013$pop, gmp2013$pcgmp)

# 输出结果
cat("2006年参数: y0 =", fit_opt1$y0, "a =", fit_opt1$a, 
    "\n2013年参数: y0 =", fit_2013$y0, "a =", fit_2013$a,
    "\n\n标准误比较:\n2006: se_y0 =", se_2006$se_y0, "se_a =", se_2006$se_a,
    "\n2013: se_y0 =", se_2013$se_y0, "se_a =", se_2013$se_a)

# 显著性检验 (z检验)
z_y0 <- (fit_opt1$y0 - fit_2013$y0) / sqrt(se_2006$se_y0^2 + se_2013$se_y0^2)
z_a <- (fit_opt1$a - fit_2013$a) / sqrt(se_2006$se_a^2 + se_2013$se_a^2)

p_y0 <- 2 * pnorm(-abs(z_y0))
p_a <- 2 * pnorm(-abs(z_a))

cat("\n假设检验:\n y0变化 p值:", format.pval(p_y0), 
    "\n a变化 p值:", format.pval(p_a))
```

